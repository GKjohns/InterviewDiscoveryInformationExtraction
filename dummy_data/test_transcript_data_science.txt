# Data Science Work Experience Research Interview
mxk-vytp-qrs (2024-10-21 14:22 GMT-4) - Transcript
Attendees: Rachel (Researcher), David (Senior Data Scientist at BigTech Corp)

Rachel: Hi David, thanks for making the time today! I'd love to hear about your experience as a data scientist at BigTech. Could you start by telling me about your role and what a typical week looks like?

David: *chuckles* A typical week? I don't think those exist anymore. But generally, I'm juggling about three main projects, five "urgent requests" that somehow all became urgent simultaneously, and at least one stakeholder who's convinced that AI can solve world hunger by Friday.

Rachel: *laughs* Sounds chaotic! Can you walk me through how you manage those competing priorities?

David: Sure! I've developed what I call the "data science triage system." Basically, I categorize requests into three buckets: "The building is on fire," "Someone thinks the building is on fire but it's actually just a candle," and "Someone saw smoke on TV and now wants us to install sprinklers everywhere." It helps me maintain my sanity.

Rachel: That's a creative categorization! How do you determine which analyses are actually "building on fire" priorities?

David: I look at three things: revenue impact, timeline sensitivity, and what I call the "executive anxiety level" - which is basically how many question marks are in their emails. *laughs* But seriously, I've learned to ask specific questions about why they need the analysis and what decisions it will influence. Sometimes what seems urgent is actually just interesting, and what seems minor could have huge business implications.

Rachel: Could you give me an example of an analysis that surprised you in terms of its impact?

David: Oh, this is a good one. Last year, we did what seemed like a pretty routine analysis of our customer support chat logs - basic stuff, looking at response times, satisfaction scores, that kind of thing. But we found this weird pattern where customer satisfaction would tank every Tuesday afternoon. Everyone assumed it was a data error at first.

Turns out, that's when we were running our system updates, which would slow down our support tools. Nobody had connected those dots before because the engineering team scheduled updates in UTC, and the support team was working in PST, and somehow these two teams had never actually talked to each other about it.

Rachel: That's fascinating! How did you present those findings to make them actionable?

David: This is where the art of data science comes in. I've learned that executives don't want to see your beautiful R code or your incredibly clever statistical analysis. They want to know three things: what's broken, why it matters, and how to fix it.

So instead of showing them my complexity analysis of the system performance degradation - which I spent way too much time on, by the way - I showed them one slide: "We're losing $50,000 every Tuesday because our computers are taking a nap." That got their attention really quick.

Rachel: *laughs* That's quite the executive summary! How do you balance technical rigor with that kind of simplified communication?

David: It's like having a mullet - business in the front, party in the back. I always have the deep technical analysis ready to go, with all the methodology and statistical validation. But I start with the business impact. I learned this after what we now call the "Great Regression Disaster of 2023."

Rachel: Oh, I have to hear about this disaster!

David: Picture this: I'm presenting to our C-suite, really proud of this complex multi-variate regression analysis I'd done. I put up this gorgeous scatter plot with five different trend lines, started explaining p-values... and watched their eyes glaze over in real time. The CFO actually fell asleep. Like, audibly snoring.

Now I start with "This analysis found $2 million in missed revenue opportunities," and suddenly everyone's wide awake and asking questions. The technical details are still there, but they're appendix material.

Rachel: That's a great learning experience! How do you influence decision-making at different levels of the organization?

David: I've developed what I call the "stakeholder pyramid of needs." For executives, everything has to tie to revenue or strategic objectives. For middle management, it's all about operational efficiency and team metrics. For the front-line teams, they want practical, actionable insights that make their daily work easier.

The trick is to frame the same analysis differently for each audience. Take that Tuesday support issue - for executives, it was about revenue impact. For the support managers, it was about team stress and turnover. For the engineers, it was about system optimization. Same data, different stories.

Rachel: That's really insightful. How do you manage stakeholders who come to you with specific requests that you know won't actually solve their problem?

David: Ah, you mean the "Can you AI this?" requests? *laughs* Those are my favorite. Last month, someone asked if we could use machine learning to predict when employees would quit. Turns out what they really needed was just to talk to their team more.

I've learned to ask "What decision are you trying to make?" before starting any analysis. Sometimes people ask for complex solutions when they really just need a simple dashboard. Or they ask for predictive models when what they actually need is basic data hygiene.

Rachel: How do you handle situations where your analysis contradicts what stakeholders believe or want to hear?

David: Very carefully, and preferably with lots of backup data! *laughs* But seriously, I've learned to present controversial findings as opportunities rather than problems. Instead of "Your favorite project is losing money," it's "Here's an opportunity to reallocate resources for better ROI."

I also always bring receipts - lots of them. The more surprising the finding, the more rigorous the validation needs to be. I learned this after what we call the "Great Mobile App Uprising of 2023."

Rachel: Another disaster story? Do tell!

David: So we did this analysis that showed our super expensive mobile app redesign had actually made user engagement worse. The product team had already done a victory lap about it, sent out press releases, the whole nine yards. When we showed them the data, they practically formed a mob with pitchforks.

But because we had done extensive A/B testing, had multiple validation approaches, and could show the exact user behaviors that changed - they couldn't argue with it. They were mad, but they couldn't argue. Now they actually come to us before making big changes, which is a win.

Rachel: How do you ensure your team's analyses are actually driving change?

David: I started keeping what I call an "impact ledger." For every analysis we do, we track three things: what changed because of our work, how much money it saved or made, and what we learned for next time. It helps us focus on work that matters and gives us ammunition for budget discussions.

I also started doing "pre-mortems" for big analyses. Before we start, we ask "If this project fails to drive change, why would that happen?" It helps us spot potential roadblocks early and plan around them.

Rachel: That's really smart! What about building data literacy in the broader organization?

David: I run what we affectionately call "Data Therapy Sessions." Every Thursday, anyone can book 30 minutes to talk through their data problems. It started as office hours for debugging SQL queries, but it's evolved into this amazing knowledge-sharing platform.

The best part is, it helps us spot patterns in what teams are struggling with. If three different people ask about the same metric in one week, that's usually a sign we need to build better documentation or tools around it.

Rachel: How do you balance the need for quick insights versus more thorough analysis?

David: I use what I call the "iceberg method." Every analysis has three layers: the quick insights you can get in a day, the deeper patterns you find in a week, and the complex relationships that take a month to untangle. The trick is knowing which layer you need for each decision.

Sometimes a rough approximation NOW is better than a perfect analysis later. But you have to be very clear about the confidence level and limitations. I always say "This is our 24-hour view, and here's what we might find if we dig deeper."

Rachel: How do you maintain quality with that kind of rapid analysis?

David: Checklists, checklists, checklists. I have them for everything. Data quality checks, methodology validation, peer review processes. It might sound bureaucratic, but it's saved us from so many potential embarrassments.

I also believe in what I call "productive paranoia." Before sending any analysis, I ask myself, "How could this make me look really stupid in front of the CEO?" It's amazing how many errors you catch when you frame it that way!

Rachel: *laughs* That's a great quality control method! As we wrap up, what advice would you give to other data scientists about making their work more influential?

David: Three things: First, learn to speak business - dollars talk, p-values walk. Second, build relationships - the best analysis in the world won't matter if nobody trusts you. And third, always have a backup of your backup - nothing builds credibility like being able to answer detailed questions on the spot.

Oh, and maybe don't let the CFO fall asleep during your presentations. Still haven't lived that one down!

Rachel: This has been incredibly insightful - and entertaining! Thank you so much for your time.

David: Happy to help! Now I need to go check if our Tuesday afternoon metrics have improved, or if we're still letting the computers take their afternoon tea break.

Meeting ended after 01:12:45

This editable transcript was computer generated and might contain errors. People can also change the text after it was created.